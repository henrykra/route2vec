{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7266d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f286297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bdb2025.py\n",
    "\"\"\"Common functions for loading and preparing Big Data Bowl 2025 data for modeling.\"\"\"\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_csv_data(\n",
    "    paths: list[Path]\n",
    ") -> list[pd.DataFrame]:\n",
    "    # TODO: look into reading from template, i.e. tracking_week_*.csv\n",
    "    return [pd.read_csv(path) for path in paths]\n",
    "\n",
    "\n",
    "def load_tracking_data(\n",
    "    weeks: Optional[int | list[int]]=None,\n",
    "    db: Optional[Path]=None,\n",
    "    folder: Optional[Path]=None,\n",
    ") -> pd.DataFrame:\n",
    "   \n",
    "    MIN_WEEK = 1\n",
    "    MAX_WEEK = 9\n",
    "\n",
    "    if not weeks:\n",
    "        # defualt to all data\n",
    "        weeks = range(MIN_WEEK, MAX_WEEK)\n",
    "    \n",
    "    if isinstance(weeks, int):\n",
    "        weeks = [weeks]\n",
    "    weeks = set(weeks)\n",
    "    \n",
    "    for week in weeks:\n",
    "        assert isinstance(week, int) and week >= MIN_WEEK and week <= MAX_WEEK, \\\n",
    "        f\"provide integer weeks between {MIN_WEEK} and {MAX_WEEK}\"\n",
    "\n",
    "    logger.info(f\"reading data from weeks: {weeks}\")\n",
    "\n",
    "    # TODO: Load from database\n",
    "    if db and db.exists():\n",
    "        return\n",
    "\n",
    "    # load data from a folderectory\n",
    "    if folder and folder.exists():\n",
    "        logger.info(f\"reading tracking from {folder}\")\n",
    "        paths = [Path(folder, f'tracking_week_{n}.csv') for n in weeks]\n",
    "\n",
    "        dfs = load_csv_data(paths)\n",
    "        logger.info(\"finshed reading tracking\")\n",
    "        return pd.concat(dfs)\n",
    "    \n",
    "    raise FileNotFoundError\n",
    "\n",
    "\n",
    "\n",
    "def load_player_data(\n",
    "    db: Optional[Path]=None,\n",
    "    folder: Optional[Path]=None,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    if db and db.exists():\n",
    "        return\n",
    "\n",
    "    if folder and folder.exists():\n",
    "        logger.info(f\"reading player data from {folder}\")\n",
    "        path = Path(folder, 'players.csv')\n",
    "\n",
    "        return pd.read_csv(path)\n",
    "    \n",
    "\n",
    "def load_play_data(\n",
    "        db: Optional[Path]=None,\n",
    "        folder: Optional[Path]=None,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    if db and db.exists():\n",
    "        return\n",
    "\n",
    "    if folder and folder.exists():\n",
    "        logger.info(f\"reading play data from {folder}\")\n",
    "        path = Path(folder, 'plays.csv')\n",
    "\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def clean_player_data(player_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger.info(\"cleaning player data\")\n",
    "    \n",
    "    # clean height\n",
    "    feet = pd.to_numeric(player_df['height'].str.split('-', expand=True)[0])\n",
    "    inches = pd.to_numeric(player_df['height'].str.split('-', expand=True)[1])\n",
    "    player_df['height_inches'] = 12 * feet + inches\n",
    "\n",
    "    # create zscore for height\n",
    "    player_df['height_z'] = (player_df['height_inches'] - player_df['height_inches'].mean()) / player_df['height_inches'].std()\n",
    "    player_df['weight_z'] = (player_df['weight'] - player_df['weight'].mean()) / player_df['weight'].std()\n",
    "\n",
    "    logger.info(\"finished cleaning player data\")\n",
    "    return player_df\n",
    "\n",
    "\n",
    "def clean_tracking_data(tracking_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger.info(\"cleaning tracking data\")\n",
    "\n",
    "    # angles to radians\n",
    "    tracking_df['o'] = ((-1 * tracking_df['o'] + 90) % 360) * np.pi / 180\n",
    "    tracking_df['dir'] = ((-1 * tracking_df['dir'] + 90) % 360) * np.pi / 180\n",
    "\n",
    "    # standardize locations to the ball snap location\n",
    "    ball = (\n",
    "        tracking_df\n",
    "        .loc[\n",
    "            (tracking_df['event'] == 'ball_snap') & \n",
    "            (tracking_df['club'] == 'football'),\n",
    "             \n",
    "            ['gameId', 'playId', 'frameId', 'x', 'y']\n",
    "        ]\n",
    "    )\n",
    "    tracking_df = tracking_df.merge(\n",
    "        ball, # ball info for starting time\n",
    "        how='left', \n",
    "        on=('gameId', 'playId'),\n",
    "        suffixes=('', '_ball')\n",
    "    )\n",
    "\n",
    "    tracking_df['x'] = tracking_df['x'] - tracking_df['x_ball']\n",
    "    tracking_df['y'] = tracking_df['y'] - tracking_df['y_ball']\n",
    "\n",
    "    # normalize play directions to the right\n",
    "    tracking_df['x'] = ((tracking_df['playDirection'] == 'left') * -2 + 1) * tracking_df['x']\n",
    "    tracking_df['o'] = ( ((tracking_df['playDirection'] == 'left') * np.pi) + tracking_df['o'] ) % (2 * np.pi) \n",
    "    tracking_df['dir'] = ( ((tracking_df['playDirection'] == 'left') * np.pi) + tracking_df['dir'] ) % (2 * np.pi) \n",
    "\n",
    "    # x and y components of orientation and velocity\n",
    "    tracking_df['ox'] = np.cos(tracking_df['o'])\n",
    "    tracking_df['oy'] = np.sin(tracking_df['o'])\n",
    "\n",
    "    tracking_df['vx'] = np.cos(tracking_df['dir']) * tracking_df['s']\n",
    "    tracking_df['vy'] = np.sin(tracking_df['dir']) * tracking_df['s']\n",
    "\n",
    "    tracking_df['stop_point'] = pd.NA\n",
    "    tracking_df.loc[tracking_df['event'].isin(['pass_outcome_incomplete', 'qb_sack', 'tackle']), 'stop_point'] = True \n",
    "    # fill stopping points forward to filter out moments after a defined route stop\n",
    "    tracking_df['stop_point'] = tracking_df.groupby(['gameId', 'playId', 'nflId'])['stop_point'].ffill()\n",
    "    \n",
    "    tracking_df = tracking_df.loc[pd.isna(tracking_df['stop_point'])].copy()\n",
    "\n",
    "    # filter\n",
    "    tracking_df = tracking_df.loc[\n",
    "        (tracking_df['frameId'] >= tracking_df['frameId_ball']) & # after the starting point\n",
    "        (tracking_df['club'] != 'football')\n",
    "        \n",
    "        #['gameId', 'playId', 'frameId', 'nflId', 'x', 'y', 'vx', 'vy', 'a', 'ox', 'oy']\n",
    "    ].copy()\n",
    "    \n",
    "    logger.info(\"finished cleaning tracking data\")\n",
    "    return tracking_df\n",
    "\n",
    "\n",
    "def mirror_tracking_plays(tracking_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger.info(\"mirroring tracking data\")\n",
    "    mirrored_df = tracking_df.copy()\n",
    "\n",
    "    mirrored_df['y'] = mirrored_df['y'] * -1\n",
    "    mirrored_df['vy'] = mirrored_df['vy'] * -1\n",
    "    mirrored_df['oy'] = mirrored_df['oy'] * -1\n",
    "\n",
    "    mirrored_df['mirrored'] = True\n",
    "    tracking_df['mirrored'] = False\n",
    "\n",
    "    logger.info(\"finished mirroring tracking data\")\n",
    "\n",
    "    return pd.concat([tracking_df, mirrored_df])\n",
    "\n",
    "\n",
    "def prepare_static_data(tracking_df: pd.DataFrame, play_df: pd.DataFrame, player_df: pd.DataFrame ) -> pd.DataFrame:\n",
    "    logger.info(\"preparing static data\")\n",
    "    # join \n",
    "\n",
    "    joined_df = (\n",
    "        tracking_df\n",
    "        .groupby(['gameId', 'playId', 'nflId', 'mirrored'], as_index=False)\n",
    "        .first()\n",
    "        .merge(\n",
    "            play_df,\n",
    "            how='left',\n",
    "            on=('gameId', 'playId')\n",
    "        )\n",
    "        .merge( # player info for positions\n",
    "            player_df, \n",
    "            how='left',\n",
    "            on='nflId'\n",
    "        )\n",
    "    )\n",
    "    joined_df['offense'] = joined_df['club'] == joined_df['possessionTeam']\n",
    "\n",
    "    # filter\n",
    "    joined_df = joined_df[        \n",
    "        ['gameId', 'playId', 'nflId', 'mirrored', 'height_z', 'weight_z', 'position', 'offense']\n",
    "    ].copy()\n",
    "\n",
    "    joined_df = pd.get_dummies(joined_df)\n",
    "    logger.info(\"finished preparing static data\")\n",
    "\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets.py\n",
    "\"\"\"Functions for perparing datasets for modeling\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def interpolate_movement(tracking_df: pd.DataFrame) -> np.ndarray:\n",
    "\n",
    "    play_index = tracking_df.set_index(['gameId', 'playId', 'mirrored']).sort_index()\n",
    "\n",
    "    num_obs = 40\n",
    "    interpolated_columns = ['x', 'y', 'ox', 'oy', 'vx', 'vy', 'a']\n",
    "    ids = play_index.index.unique()\n",
    "    num_plays = len(ids)\n",
    "    routes_per_play = tracking_df.groupby(['gameId', 'playId'])['nflId'].nunique(dropna=True).max()\n",
    "\n",
    "    logger.info(f\"interpolating data {len(interpolated_columns)} of data\")\n",
    "    # array for interpolated movement\n",
    "    movement_arr = np.zeros( (num_plays, routes_per_play, num_obs, len(interpolated_columns)), dtype=np.float32)\n",
    "\n",
    "    for i, (gameid, playid, mirrored) in enumerate(ids): \n",
    "        \n",
    "        play: pd.DataFrame = play_index.loc[gameid, playid, mirrored]\n",
    "        \n",
    "        for ii, nflid in enumerate(play['nflId'].dropna().unique()):\n",
    "\n",
    "            player: pd.DataFrame = play.loc[play['nflId'] == nflid]\n",
    "            min_frame = player['frameId'].min()\n",
    "            frame_range = player['frameId'].max() - min_frame\n",
    "            t = np.array((player['frameId'] - min_frame) / frame_range)\n",
    "\n",
    "            player_data = np.array(player[interpolated_columns])\n",
    "\n",
    "            # smooth the info about the player over time\n",
    "            spline = CubicSpline(t, player_data, axis=0, extrapolate=False)\n",
    "\n",
    "            # take n observations\n",
    "            observation_times = np.arange(num_obs) / num_obs\n",
    "\n",
    "            # populate an array with smoothed values\n",
    "            movement_arr[i, ii, ...] = spline(observation_times)\n",
    "\n",
    "    logger.info(\"finished interpolating data\")\n",
    "\n",
    "    return movement_arr\n",
    "\n",
    "\n",
    "def perpare_data_arrays(static_df: pd.DataFrame, movement_arr: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    logger.info(\"preparing arrays\")\n",
    "    play_index = static_df.set_index(['gameId', 'playId', 'mirrored'])\n",
    "    \n",
    "    num_plays = len(play_index.index.unique())\n",
    "    routes_per_play = static_df.groupby(['gameId', 'playId', 'mirrored'])['nflId'].nunique(dropna=True).iloc[0]\n",
    "\n",
    "    assert (static_df.groupby(['gameId', 'playId', 'mirrored'])['nflId'].nunique(dropna=False) == routes_per_play).all(), \\\n",
    "    \"Different values across player dimension\"\n",
    "    \n",
    "    num_columns = len(play_index.columns) - 1 # subtract the nflId column\n",
    "    player_info = np.zeros( (num_plays, routes_per_play, num_columns), dtype=np.float32) \n",
    "\n",
    "    for i, (name, group) in enumerate(play_index.groupby(['gameId', 'playId', 'mirrored'])):\n",
    "        player_info[i, ...] = group.drop(columns=['nflId'])\n",
    "\n",
    "    input_arr = np.concat(\n",
    "        [\n",
    "            movement_arr.reshape((num_plays, routes_per_play, -1)), # combine final axes \n",
    "            player_info, # concat fixed info\n",
    "        ],\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    target_arr = movement_arr[..., :2].reshape((num_plays, routes_per_play, -1))\n",
    "\n",
    "    logger.info(\"finished preparing arrays\")\n",
    "    return input_arr, target_arr\n",
    "\n",
    "\n",
    "def mask_input(input_arr: np.ndarray, n: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    logger.info(\"masking input\")\n",
    "    mask_pct = .15\n",
    "\n",
    "    # \"mask token\"\n",
    "    mask = np.zeros(n)\n",
    "\n",
    "    n_player_vectors = np.prod(input_arr.shape[:-1])\n",
    "    mask_idx = np.random.choice(\n",
    "        n_player_vectors,\n",
    "        size=int(n_player_vectors * mask_pct),\n",
    "        replace=False\n",
    "    ) # random choice over the product of the first axes\n",
    "\n",
    "    # determine vectors to mask\n",
    "\n",
    "    masked_players_arr = input_arr.reshape((n_player_vectors, -1)).copy()\n",
    "    masked_players_arr[mask_idx, :n] = mask # TODO: test with 0 for readibility\n",
    "    mask_arr = np.zeros(masked_players_arr.shape[0], dtype=np.float32)\n",
    "    mask_arr[mask_idx] = 1\n",
    "\n",
    "    logger.info(\"finished masking input\")\n",
    "\n",
    "    return masked_players_arr.reshape(input_arr.shape), mask_arr.reshape(input_arr.shape[:2])\n",
    "\n",
    "\n",
    "def train_val_test_split(n: int):\n",
    "\n",
    "    idx = np.arange(n)\n",
    "\n",
    "    cats = np.random.choice(a=[0, 1, 2], size=n, replace=True, p=[.7, .15, .15]) # 70, 15, 15\n",
    "\n",
    "    split = np.stack([idx, cats], axis=-1)\n",
    "\n",
    "    train_idx = split[split[:, -1] == 0, 0]\n",
    "    val_idx = split[split[:, -1] == 1, 0]\n",
    "    test_idx = split[split[:, -1] == 2, 0]\n",
    "\n",
    "    return train_idx, val_idx, test_idx\n",
    "    \n",
    "\n",
    "class NflDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_arr: np.ndarray, target_arr: np.ndarray, mask_arr: np.ndarray, idx: np.ndarray, device: torch.DeviceObjType=torch.device(\"cpu\")):\n",
    "\n",
    "        # make sure there's the right amount of input observatinos and target observations\n",
    "        assert input_arr.shape[0] == target_arr.shape[0], \"# Observations mismatch between input and target\"\n",
    "\n",
    "        self.input_t: torch.Tensor = torch.Tensor(input_arr[idx].copy()).to(device)\n",
    "        self.target_t: torch.Tensor = torch.Tensor(target_arr[idx].copy()).to(device)\n",
    "        self.mask_t: torch.Tensor = torch.Tensor(mask_arr[idx].copy()).to(device)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        return self.input_t[idx], self.target_t[idx], self.mask_t[idx].unsqueeze(-1)\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "\n",
    "        return self.input_t.shape[0]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\"\"\"Model Architecture\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class NflBERT(torch.nn.Module):\n",
    "    \"\"\"Bert style model for encoding nfl player movement\n",
    "    \n",
    "    BERT uses a nearly identical transformer encoder as \"Attention is all you need\" and thus\n",
    "    similar to the provided torch TransformerEncoder layer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        output_dim: int,\n",
    "        hidden_size: int=512,\n",
    "        num_layers: int=8,\n",
    "        num_heads: int=8,\n",
    "        ffn_size: int=2048,\n",
    "        ffn_act: str=\"gelu\",\n",
    "        dropout: float=.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm_layer = torch.nn.BatchNorm1d(feature_dim)\n",
    "        \n",
    "        self.embed = torch.nn.Sequential(\n",
    "            torch.nn.Linear(feature_dim, hidden_size),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.LayerNorm(hidden_size),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_size,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=ffn_size,\n",
    "                dropout=dropout,\n",
    "                activation=ffn_act,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, hidden_size // 4),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.LayerNorm(hidden_size // 4),\n",
    "            torch.nn.Linear(hidden_size // 4, output_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \n",
    "        # x: [B: batch_size, P: # of players, F: feature_len]\n",
    "        B, P, F = x.size()\n",
    "\n",
    "        # Normalize features\n",
    "        x = self.norm_layer(x.permute(0, 2, 1)).permute(0, 2, 1)  # [B,P,F] -> [B,P,F]\n",
    "\n",
    "        # Embed features\n",
    "        x = self.embed(x)  # [B,P,F] -> [B,P,M: model_dim]\n",
    "\n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer(x)  # [B,P,M] -> [B,P,M]\n",
    "\n",
    "        # Decode to predict tackle location\n",
    "        x = self.decoder(x)  # [B, P, M] -> [B,P,O]\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605f4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "bdb_folder = Path(\"/Users/henrykraessig/code/bdb2025/data/\")\n",
    "\n",
    "tracking = pd.read_csv(\"minimal_tracking.csv\")\n",
    "tracking = load_tracking_data(folder=bdb_folder)\n",
    "players = load_player_data(folder=bdb_folder)\n",
    "plays = load_play_data(folder=bdb_folder)\n",
    "\n",
    "players = clean_player_data(players)\n",
    "\n",
    "tracking = clean_tracking_data(tracking)\n",
    "tracking = mirror_tracking_plays(tracking)\n",
    "\n",
    "static = prepare_static_data(tracking, plays, players)\n",
    "num_static_cols = len([col for col in static.columns if 'Id' not in col and 'mirrored' not in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "movement = interpolate_movement(tracking)\n",
    "input, target= perpare_data_arrays(static, movement)\n",
    "input, mask = mask_input(input, np.prod(movement.shape[2:]))\n",
    "\n",
    "train_idx, val_idx, test_idx = train_val_test_split(input.shape[0])\n",
    "\n",
    "\n",
    "train_dataset = NflDataset(input, target, mask, train_idx)\n",
    "val_dataset = NflDataset(input, target, mask, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed780b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data eval & plotting to debug\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_anim(inp: np.ndarray, tar: np.ndarray, idx: int, static_cols: int, interp: int):\n",
    "    \n",
    "    # inp: [x, 22, 303]\n",
    "\n",
    "    play = inp[idx, :, :-static_cols].reshape((inp.shape[1], interp, -1)) # 22, 40, 7\n",
    "    play = play[..., :2] # 22, 40, 2\n",
    "\n",
    "    ids = np.repeat(range(play.shape[0]), play.shape[1]).reshape(list(play.shape[:-1]) + [1]) # 22, 40, 1\n",
    "    frames = np.tile(range(play.shape[1]), play.shape[0]).reshape(list(play.shape[:-1]) + [1])\n",
    "\n",
    "\n",
    "    # target\n",
    "    tar_filter = ~play.all(axis=(-1, -2))\n",
    "    target = tar[idx, tar_filter] # ?, 80\n",
    "    \n",
    "    target = target.reshape((-1, interp, 2))\n",
    "\n",
    "    play[tar_filter, ...] = target\n",
    "    ids[tar_filter, ...] = -1\n",
    "\n",
    "    play_id = np.concat([play, ids, frames], axis=-1)\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame(play_id.reshape((-1, play_id.shape[-1])), columns=['x', 'y', 'id', 'frame'])\n",
    "    df['target'] = (df['id'] == -1)\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        color='target',\n",
    "        animation_frame=\"frame\",\n",
    "        title=f\"test\"\n",
    "    )\n",
    "\n",
    "\n",
    "    return fig\n",
    "\n",
    "plot_anim(input.copy(), target.copy(), 0, num_static_cols, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_eval(tar: np.ndarray, pred: np.ndarray, mask: np.ndarray, idx: int, interp: int):\n",
    "    \n",
    "    if mask.shape[-1] == 1:\n",
    "        mask = mask.squeeze(axis=-1)\n",
    "\n",
    "    # static (not animated) plotly plot to visualize model predictions against groundtruth\n",
    "\n",
    "    # plot the first of the entire input array\n",
    "    targets = tar.reshape(*tar.shape[:-1], interp, -1)\n",
    "    predictions = pred.reshape(*pred.shape[:-1], interp, -1)\n",
    "\n",
    "    starts = targets[idx, :, 0, :2] # first frame of x&y for 22 players\n",
    "\n",
    "    # get the masked player's movements\n",
    "    gt_movement = targets[idx, mask[idx].astype(np.bool)]\n",
    "\n",
    "    # get predicted movements\n",
    "    pred_movement = predictions[idx, mask[idx].astype(np.bool)]\n",
    "\n",
    "\n",
    "    # plot start locations and lines for gt and pred movement\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace( # start locations\n",
    "        go.Scatter(\n",
    "            y = starts[..., 0],\n",
    "            x = starts[..., 1],\n",
    "            mode = 'markers', \n",
    "            showlegend=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i, (player_gt, player_pred) in enumerate(zip(gt_movement, pred_movement)):\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                y = player_gt[..., 0],\n",
    "                x = player_gt[..., 1],\n",
    "                name = f\"groundtruth {i}\",\n",
    "                showlegend = False,\n",
    "                mode = 'lines',\n",
    "                marker = dict(\n",
    "                    color = 'black'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                y = player_pred[..., 0],\n",
    "                x = player_pred[..., 1],\n",
    "                name = f\"predicted {i}\",\n",
    "                showlegend = False,\n",
    "                mode = 'lines',\n",
    "                marker = dict(\n",
    "                    color = 'red'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_heads\": 2,\n",
    "    \"ffn_size\": 64 * 4,\n",
    "    \"epochs\": 100,\n",
    "    \"lr\": 0.001,\n",
    "    \"patience\": 10,\n",
    "    \"device\": \"cpu\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, hparams[\"batch_size\"], True)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, hparams[\"batch_size\"], False)\n",
    "model = NflBERT(\n",
    "    input.shape[-1], # in \n",
    "    target.shape[-1], # out\n",
    "    hidden_size=hparams[\"hidden_size\"],\n",
    "    num_layers=hparams[\"num_layers\"], \n",
    "    num_heads=hparams[\"num_heads\"], \n",
    "    ffn_size=hparams[\"ffn_size\"]\n",
    ").to(hparams[\"device\"])\n",
    "\n",
    "loss_fn = torch.nn.SmoothL1Loss()\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=hparams[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30780cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the loss function\n",
    "# create the optimizer\n",
    "# create plotting utils\n",
    "# analyze the play normailzation - vanishing gradients?\n",
    "# fix masking to only loss over masked samples & limit max masked samples per play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edafe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "\n",
    "writer = SummaryWriter('./runs/allweeks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_early_stopping(loss: float, best_val_loss: float, no_improvement: int, patience: int) -> tuple[bool, float, int]:\n",
    "    if loss < best_val_loss:\n",
    "        return True, loss, 0 # reset no improvement\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        if no_improvement >= patience:\n",
    "            return False, best_val_loss, patience\n",
    "        else:\n",
    "            return True, best_val_loss, no_improvement\n",
    "        \n",
    "\n",
    "for epoch in tqdm(range(hparams[\"epochs\"])):\n",
    "\n",
    "    c = 0\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    no_improvement = 0\n",
    "\n",
    "    model.train()\n",
    "    for (inp, tar, mask) in train_loader:\n",
    "\n",
    "        optim.zero_grad()\n",
    "        x = model(inp)\n",
    "\n",
    "        loss = loss_fn(mask*x, mask*tar)\n",
    "        train_loss += loss\n",
    "        c += 1\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "    # log average loss\n",
    "    writer.add_scalar('Loss/train', train_loss / c, epoch)\n",
    "\n",
    "    c = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (inp, tar, mask) in val_loader:\n",
    "            x = model(inp)\n",
    "\n",
    "            loss = loss_fn(x*mask, tar*mask)\n",
    "            val_loss += loss\n",
    "            c += 1\n",
    "\n",
    "    # log average loss\n",
    "        writer.add_scalar('Loss/val', val_loss / c, epoch)\n",
    "\n",
    "        cont, best_val_loss, no_improvement = check_early_stopping(val_loss, best_val_loss, no_improvement, hparams[\"patience\"])\n",
    "        if not cont: # early stopped\n",
    "            print(f\"Early stopping at epoch {epoch}\\nBest Loss: {best_val_loss}\")\n",
    "            break\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            # log a sample play\n",
    "            pred = model(inp)\n",
    "            \n",
    "            fig = plot_eval(tar.numpy().copy(), pred.numpy().copy(), mask.numpy().copy(), 0, 40)\n",
    "            img_bytes = fig.to_image(format='jpg')\n",
    "            img = Image.open(io.BytesIO(img_bytes))\n",
    "            \n",
    "            writer.add_image(f\"sample/{epoch}\", np.array(img).transpose([2, 0, 1]))\n",
    "\n",
    "            print(f\"Epoch: {epoch}\\tVal Loss: {val_loss/c:.3f}\")\n",
    "        \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcec71b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
