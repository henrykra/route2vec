{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f286297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bdb2025.py\n",
    "\"\"\"Common functions for loading and preparing Big Data Bowl 2025 data for modeling.\"\"\"\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_csv_data(\n",
    "    paths: list[Path]\n",
    ") -> list[pd.DataFrame]:\n",
    "    # TODO: look into reading from template, i.e. tracking_week_*.csv\n",
    "    return [pd.read_csv(path) for path in paths]\n",
    "\n",
    "\n",
    "def load_tracking_data(\n",
    "    weeks: Optional[int | list[int]]=None,\n",
    "    db: Optional[Path]=None,\n",
    "    folder: Optional[Path]=None,\n",
    ") -> pd.DataFrame:\n",
    "   \n",
    "    MIN_WEEK = 1\n",
    "    MAX_WEEK = 9\n",
    "\n",
    "    if not weeks:\n",
    "        # defualt to all data\n",
    "        weeks = range(MIN_WEEK, MAX_WEEK)\n",
    "    \n",
    "    if isinstance(weeks, int):\n",
    "        weeks = [weeks]\n",
    "    weeks = set(weeks)\n",
    "    \n",
    "    for week in weeks:\n",
    "        assert isinstance(week, int) and week >= MIN_WEEK and week <= MAX_WEEK, \\\n",
    "        f\"provide integer weeks between {MIN_WEEK} and {MAX_WEEK}\"\n",
    "\n",
    "    # TODO: Load from database\n",
    "    if db and db.exists():\n",
    "        return\n",
    "\n",
    "    # load data from a folderectory\n",
    "    if folder and folder.exists():\n",
    "        paths = [Path(folder, f'tracking_week_{n}.csv') for n in weeks]\n",
    "\n",
    "        dfs = load_csv_data(paths)\n",
    "        return pd.concat(dfs)\n",
    "    \n",
    "    raise FileNotFoundError\n",
    "\n",
    "\n",
    "\n",
    "def load_player_data(\n",
    "    db: Optional[Path]=None,\n",
    "    folder: Optional[Path]=None,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    if db and db.exists():\n",
    "        return\n",
    "\n",
    "    if folder and folder.exists():\n",
    "        path = Path(folder, 'players.csv')\n",
    "\n",
    "        return pd.read_csv(path)\n",
    "    \n",
    "\n",
    "def load_play_data(\n",
    "        db: Optional[Path]=None,\n",
    "        folder: Optional[Path]=None,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    if db and db.exists():\n",
    "        return\n",
    "\n",
    "    if folder and folder.exists():\n",
    "        path = Path(folder, 'plays.csv')\n",
    "\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def clean_player_data(player_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # clean height\n",
    "    feet = pd.to_numeric(player_df['height'].str.split('-', expand=True)[0])\n",
    "    inches = pd.to_numeric(player_df['height'].str.split('-', expand=True)[1])\n",
    "    player_df['height_inches'] = 12 * feet + inches\n",
    "\n",
    "    # create zscore for height\n",
    "    player_df['height_z'] = (player_df['height_inches'] - player_df['height_inches'].mean()) / player_df['height_inches'].std()\n",
    "    player_df['weight_z'] = (player_df['weight'] - player_df['weight'].mean()) / player_df['weight'].std()\n",
    "\n",
    "    return player_df\n",
    "\n",
    "\n",
    "def clean_tracking_data(tracking_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # angles to radians\n",
    "    tracking_df['o'] = ((-1 * tracking_df['o'] + 90) % 360) * np.pi / 180\n",
    "    tracking_df['dir'] = ((-1 * tracking_df['dir'] + 90) % 360) * np.pi / 180\n",
    "\n",
    "    # standardize locations to the ball snap location\n",
    "    ball = (\n",
    "        tracking_df\n",
    "        .loc[\n",
    "            (tracking_df['event'] == 'ball_snap') & \n",
    "            (tracking_df['club'] == 'football'),\n",
    "             \n",
    "            ['gameId', 'playId', 'frameId', 'x', 'y']\n",
    "        ]\n",
    "    )\n",
    "    tracking_df = tracking_df.merge(\n",
    "        ball, # ball info for starting time\n",
    "        how='left', \n",
    "        on=('gameId', 'playId'),\n",
    "        suffixes=('', '_ball')\n",
    "    )\n",
    "\n",
    "    tracking_df['x'] = tracking_df['x'] - tracking_df['x_ball']\n",
    "    tracking_df['y'] = tracking_df['y'] - tracking_df['y_ball']\n",
    "\n",
    "    # normalize play directions to the right\n",
    "    tracking_df['x'] = ((tracking_df['playDirection'] == 'left') * -2 + 1) * tracking_df['x']\n",
    "    tracking_df['o'] = ( ((tracking_df['playDirection'] == 'left') * np.pi) + tracking_df['o'] ) % (2 * np.pi) \n",
    "    tracking_df['dir'] = ( ((tracking_df['playDirection'] == 'left') * np.pi) + tracking_df['dir'] ) % (2 * np.pi) \n",
    "\n",
    "    # x and y components of orientation and velocity\n",
    "    tracking_df['ox'] = np.cos(tracking_df['o'])\n",
    "    tracking_df['oy'] = np.sin(tracking_df['o'])\n",
    "\n",
    "    tracking_df['vx'] = np.cos(tracking_df['dir']) * tracking_df['s']\n",
    "    tracking_df['vy'] = np.sin(tracking_df['dir']) * tracking_df['s']\n",
    "\n",
    "    tracking_df['stop_point'] = pd.NA\n",
    "    tracking_df.loc[tracking_df['event'].isin(['pass_outcome_incomplete', 'qb_sack', 'tackle']), 'stop_point'] = True \n",
    "    # fill stopping points forward to filter out moments after a defined route stop\n",
    "    tracking_df['stop_point'] = tracking_df.groupby(['gameId', 'playId', 'nflId'])['stop_point'].ffill()\n",
    "    \n",
    "    tracking_df = tracking_df.loc[pd.isna(tracking_df['stop_point'])].copy()\n",
    "\n",
    "    # filter\n",
    "    tracking_df = tracking_df.loc[\n",
    "        (tracking_df['frameId'] >= tracking_df['frameId_ball']) & # after the starting point\n",
    "        (tracking_df['club'] != 'football')\n",
    "        \n",
    "        #['gameId', 'playId', 'frameId', 'nflId', 'x', 'y', 'vx', 'vy', 'a', 'ox', 'oy']\n",
    "    ].copy()\n",
    "    \n",
    "    return tracking_df\n",
    "\n",
    "\n",
    "def mirror_tracking_plays(tracking_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    mirrored_df = tracking_df.copy()\n",
    "\n",
    "    mirrored_df['y'] = mirrored_df['y'] * -1\n",
    "    mirrored_df['vy'] = mirrored_df['vy'] * -1\n",
    "    mirrored_df['oy'] = mirrored_df['oy'] * -1\n",
    "\n",
    "    mirrored_df['mirrored'] = True\n",
    "    tracking_df['mirrored'] = False\n",
    "\n",
    "    return pd.concat([tracking_df, mirrored_df])\n",
    "\n",
    "\n",
    "def prepare_static_data(tracking_df: pd.DataFrame, play_df: pd.DataFrame, player_df: pd.DataFrame ) -> pd.DataFrame:\n",
    "\n",
    "    # join \n",
    "    joined_df = (\n",
    "        tracking_df\n",
    "        .groupby(['gameId', 'playId', 'nflId', 'mirrored'], as_index=False)\n",
    "        .first()\n",
    "        .merge(\n",
    "            play_df,\n",
    "            how='left',\n",
    "            on=('gameId', 'playId')\n",
    "        )\n",
    "        .merge( # player info for positions\n",
    "            player_df, \n",
    "            how='left',\n",
    "            on='nflId'\n",
    "        )\n",
    "    )\n",
    "    joined_df['offense'] = joined_df['club'] == joined_df['possessionTeam']\n",
    "\n",
    "    # filter\n",
    "    joined_df = joined_df[        \n",
    "        ['gameId', 'playId', 'frameId', 'nflId', 'mirrored', 'height_z', 'weight_z', 'position', 'offense']\n",
    "    ].copy()\n",
    "\n",
    "    joined_df = pd.get_dummies(joined_df)\n",
    "\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets.py\n",
    "\"\"\"Functions for perparing datasets for modeling\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def interpolate_movement(tracking_df: pd.DataFrame) -> np.ndarray:\n",
    "\n",
    "    play_index = tracking_df.set_index(['gameId', 'playId', 'mirrored']).sort_index()\n",
    "\n",
    "    num_obs = 40\n",
    "    interpolated_columns = ['x', 'y', 'ox', 'oy', 'vx', 'vy', 'a']\n",
    "    ids = play_index.index.unique()\n",
    "    num_plays = len(ids)\n",
    "    routes_per_play = tracking_df.groupby(['gameId', 'playId'])['nflId'].nunique(dropna=True).max()\n",
    "\n",
    "    # array for interpolated movement\n",
    "    movement_arr = np.zeros( (num_plays, routes_per_play, num_obs, len(interpolated_columns)), dtype=np.float32)\n",
    "\n",
    "    for i, (gameid, playid, mirrored) in enumerate(ids): \n",
    "        \n",
    "        play: pd.DataFrame = play_index.loc[gameid, playid, mirrored]\n",
    "        \n",
    "        for ii, nflid in enumerate(play['nflId'].dropna().unique()):\n",
    "\n",
    "            player: pd.DataFrame = play.loc[play['nflId'] == nflid]\n",
    "            min_frame = player['frameId'].min()\n",
    "            frame_range = player['frameId'].max() - min_frame\n",
    "            t = np.array((player['frameId'] - min_frame) / frame_range)\n",
    "\n",
    "            player_data = np.array(player[interpolated_columns])\n",
    "\n",
    "            # smooth the info about the player over time\n",
    "            spline = CubicSpline(t, player_data, axis=0, extrapolate=False)\n",
    "\n",
    "            # take n observations\n",
    "            observation_times = np.arange(num_obs) / num_obs\n",
    "\n",
    "            # populate an array with smoothed values\n",
    "            movement_arr[i, ii, ...] = spline(observation_times)\n",
    "\n",
    "    return movement_arr\n",
    "\n",
    "\n",
    "def perpare_data_arrays(static_df: pd.DataFrame, movement_arr: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    play_index = static_df.set_index(['gameId', 'playId', 'mirrored'])\n",
    "    \n",
    "    num_plays = len(play_index.index.unique())\n",
    "    routes_per_play = static_df.groupby(['gameId', 'playId', 'mirrored'])['nflId'].nunique(dropna=True).iloc[0]\n",
    "\n",
    "    assert (static_df.groupby(['gameId', 'playId', 'mirrored'])['nflId'].nunique(dropna=False) == routes_per_play).all(), \\\n",
    "    \"Different values across player dimension\"\n",
    "    \n",
    "    num_columns = len(play_index.columns) - 1 # subtract the nflId column\n",
    "    player_info = np.zeros( (num_plays, routes_per_play, num_columns) ) \n",
    "\n",
    "    for i, (name, group) in enumerate(play_index.groupby(['gameId', 'playId', 'mirrored'])):\n",
    "        player_info[i, ...] = group.drop(columns=['nflId'])\n",
    "\n",
    "    input_arr = np.concat(\n",
    "        [\n",
    "            movement_arr.reshape((num_plays, routes_per_play, -1)), # combine final axes \n",
    "            player_info, # concat fixed info\n",
    "        ],\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    target_arr = movement_arr[..., :2].reshape((num_plays, routes_per_play, -1))\n",
    "\n",
    "    return input_arr, target_arr\n",
    "\n",
    "\n",
    "def mask_input(input_arr: np.ndarray, n: int) -> np.ndarray:\n",
    "    mask_pct = .15\n",
    "\n",
    "    # \"mask token\"\n",
    "    mask = np.zeros(n)\n",
    "\n",
    "    n_player_vectors = np.prod(input_arr.shape[:-1])\n",
    "    mask_idx = np.random.choice(\n",
    "        n_player_vectors,\n",
    "        size=int(n_player_vectors * mask_pct),\n",
    "        replace=False\n",
    "    ) # random choice over the product of the first axes\n",
    "\n",
    "    # determine vectors to mask\n",
    "\n",
    "    masked_players_arr = input_arr.reshape((n_player_vectors, -1)).copy()\n",
    "    masked_players_arr[mask_idx, :n] = mask\n",
    "\n",
    "    masked_players_arr.reshape(input_arr.shape)\n",
    "\n",
    "    return masked_players_arr\n",
    "\n",
    "\n",
    "class NflDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_arr: np.ndarray, target_arr: np.ndarray):\n",
    "\n",
    "        # make sure there's the right amount of input observatinos and target observations\n",
    "        assert input_arr.shape[0] == target_arr.shape[0], \"# Observations mismatch between input and target\"\n",
    "\n",
    "        self.input_arr = input_arr\n",
    "        self.target_arr = target_arr\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __getitem__(self, idx) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \n",
    "        return np.float32(self.input_arr[idx]), self.target_arr[idx]\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "\n",
    "        return self.input_arr.shape[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "128c6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\"\"\"Model Architecture\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class NflBERT(torch.nn.Module):\n",
    "    \"\"\"Bert style model for encoding nfl player movement\n",
    "    \n",
    "    BERT uses a nearly identical transformer encoder as \"Attention is all you need\" and thus\n",
    "    similar to the provided torch TransformerEncoder layer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        output_dim: int,\n",
    "        hidden_size: int=512,\n",
    "        num_layers: int=8,\n",
    "        num_heads: int=8,\n",
    "        ffn_size: int=2048,\n",
    "        ffn_act: str=\"gelu\",\n",
    "        dropout: float=.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm_layer = torch.nn.BatchNorm1d(feature_dim)\n",
    "        \n",
    "        self.embed = torch.nn.Sequential(\n",
    "            torch.nn.Linear(feature_dim, hidden_size),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.LayerNorm(hidden_size),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_size,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=ffn_size,\n",
    "                dropout=dropout,\n",
    "                activation=ffn_act,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, hidden_size // 4),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.LayerNorm(hidden_size // 4),\n",
    "            torch.nn.Linear(hidden_size // 4, output_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \n",
    "        # x: [B: batch_size, P: # of players, F: feature_len]\n",
    "        B, P, F = x.size()\n",
    "\n",
    "        # Normalize features\n",
    "        x = self.norm_layer(x.permute(0, 2, 1)).permute(0, 2, 1)  # [B,P,F] -> [B,P,F]\n",
    "\n",
    "        # Embed features\n",
    "        x = self.embed(x)  # [B,P,F] -> [B,P,M: model_dim]\n",
    "\n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer(x)  # [B,P,M] -> [B,P,M]\n",
    "\n",
    "        # Decode to predict tackle location\n",
    "        x = self.decoder(x)  # [B, P, M] -> [B,P,O]\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "605f4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "bdb_folder = Path(\"/Users/henrykraessig/code/bdb2025/data/\")\n",
    "\n",
    "tracking = pd.read_csv(\"minimal_tracking.csv\")\n",
    "players = load_player_data(folder=bdb_folder)\n",
    "plays = load_play_data(folder=bdb_folder)\n",
    "\n",
    "players = clean_player_data(players)\n",
    "\n",
    "tracking = clean_tracking_data(tracking)\n",
    "tracking = mirror_tracking_plays(tracking)\n",
    "\n",
    "static = prepare_static_data(tracking, plays, players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a672311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "movement = interpolate_movement(tracking)\n",
    "input, target = perpare_data_arrays(static, movement)\n",
    "dataset = NflDataset(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e3eb3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, 32, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0f8d34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NflBERT(input.shape[-1], target.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30780cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the loss function\n",
    "# create the optimizer\n",
    "# split the data\n",
    "# create plotting utils\n",
    "# analyze the play normailzation\n",
    "# fix masking to only loss over masked samples & limit max masked samples per play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 22, 303]) -> torch.Size([32, 22, 80])\n",
      "torch.Size([32, 22, 80])\n",
      "torch.Size([32, 22, 303]) -> torch.Size([32, 22, 80])\n",
      "torch.Size([32, 22, 80])\n",
      "torch.Size([32, 22, 303]) -> torch.Size([32, 22, 80])\n",
      "torch.Size([32, 22, 80])\n",
      "torch.Size([32, 22, 303]) -> torch.Size([32, 22, 80])\n",
      "torch.Size([32, 22, 80])\n",
      "torch.Size([32, 22, 303]) -> torch.Size([32, 22, 80])\n",
      "torch.Size([32, 22, 80])\n",
      "torch.Size([32, 22, 303]) -> torch.Size([32, 22, 80])\n",
      "torch.Size([32, 22, 80])\n",
      "torch.Size([2, 22, 303]) -> torch.Size([2, 22, 80])\n",
      "torch.Size([2, 22, 80])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for inp, tar in loader:\n",
    "\n",
    "\n",
    "    print(f\"{inp.shape} -> {tar.shape}\")\n",
    "    print(model(inp).shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a90385d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
